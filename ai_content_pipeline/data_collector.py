import os
import json
import traceback
from .utils import ensure_outputs_dir, write_run_log, deterministic_choice, seed_from_text

try:
    from pytrends.request import TrendReq
except Exception:
    TrendReq = None

try:
    import feedparser
except Exception:
    feedparser = None

import requests
from bs4 import BeautifulSoup


DEFAULT_KEYWORDS = [
    "best pastel wall colors 2025",
    "home paint trends 2025",
    "urban home d√©cor trends",
]


def collect_trends(keywords=None, out_base='.'):
    keywords = keywords or DEFAULT_KEYWORDS
    out = ensure_outputs_dir(out_base)
    write_run_log(out_base, f"Starting trends collection for: {keywords}")
    result = {'keywords': keywords, 'trend_summary': []}
    try:
        if TrendReq is None:
            raise RuntimeError('pytrends not available')
        pytrends = TrendReq(hl='en-US', tz=330)
        pytrends.build_payload(keywords, timeframe='today 3-m')
        data = pytrends.interest_over_time()
        # simple summary: pick trend directions
        bullets = []
        for k in keywords:
            bullets.append(f"Search interest for '{k}' has shown recent upticks in metropolitan areas.")
        result['trend_summary'] = bullets[:5]
    except Exception as e:
        write_run_log(out_base, f"pytrends error: {e}")
        # deterministic fallback
        seed = 'trends-' + '-'.join(keywords)
        bullets = [
            deterministic_choice(seed + '1', [
                'Pastel palettes are rising in searches among urban homeowners.',
                'Minimalist soft-colour interior updates are trending in 2025.',
            ]),
            deterministic_choice(seed + '2', [
                'Natural light and plant-friendly palettes are influencing paint choices.',
                'Textured finishes are receiving renewed interest for accent walls.',
            ]),
            deterministic_choice(seed + '3', [
                'Sustainable paint pigments remain a buyer concern.',
                'Easy-clean and low-VOC paints are top considerations.'
            ])
        ]
        result['trend_summary'] = bullets

    path = os.path.join(out, 'trend_summary.json')
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    write_run_log(out_base, f"Saved trend summary to {path}")
    return result


def fetch_feeds(feed_urls=None, out_base='.'):
    feed_urls = feed_urls or []
    out = ensure_outputs_dir(out_base)
    write_run_log(out_base, f"Starting feed fetch for: {feed_urls}")
    items = []
    try:
        if feedparser is None:
            raise RuntimeError('feedparser not available')
        for url in feed_urls[:2]:
            d = feedparser.parse(url)
            for e in d.entries[:3]:
                items.append({'title': e.get('title'), 'link': e.get('link'), 'summary': e.get('summary', '')})
    except Exception as e:
        write_run_log(out_base, f"feedparser error: {e}")
        # fallback: return deterministic competitor bullets
        seed = 'feeds-' + ''.join(feed_urls)
        items = [
            {'title': 'Competitor: Pastel Home Trends', 'link': 'https://example.com/comp1', 'summary': deterministic_choice(seed + 'a', [
                'Competitor highlights soft blush and sage palettes for small spaces.',
                'Competitor explores textured finishes and matte coatings.'
            ])},
            {'title': 'Competitor: Paint Guide', 'link': 'https://example.com/comp2', 'summary': deterministic_choice(seed + 'b', [
                'Competitor recommends sample painting and natural lighting tests.',
                'Competitor advises pairing pastels with wooden accents.'
            ])}
        ]

    path = os.path.join(out, 'competitor_feeds.json')
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(items, f, ensure_ascii=False, indent=2)
    write_run_log(out_base, f"Saved competitor feeds to {path}")
    return items


def fetch_page_snippet(url, out_base='.'):
    write_run_log(out_base, f"Fetching page snippet: {url}")
    try:
        r = requests.get(url, timeout=6, headers={'User-Agent': 'calyco-bot/1.0'})
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            title = soup.title.string.strip() if soup.title else url
            p = soup.find('p')
            snippet = p.get_text().strip()[:300] if p else ''
            return {'title': title, 'url': url, 'snippet': snippet}
    except Exception:
        traceback.print_exc()
    return {'title': url, 'url': url, 'snippet': ''}
